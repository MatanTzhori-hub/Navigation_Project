1. Overfitting on 1 sample.
places could be good for finding bug:

1.1
- normalize data properly- MinMax normalization for each feature or gaussian normalizaion
- file path MLP\dataset_load.py
mybe no need because DataLoader do it automatically- check it

1.2
- make sure the theta % 2*pi works fine in model training and in error calc

1.3 
does batch_loss.backward() works properly? no reference for variable and function in MLP\training.py\train_batch

1.4
didnt understand what is the meaning of :
    p = itertools.product(enumerate(["train", "test"]), enumerate(["loss", "xy_dist", "theta_diff"]))
    for (i, traintest), (j, loss_avg) in p:

in mlp_wrapper.py\plot_fit

1.5  
mlp_wrapper\main - does dims = [dim] * j + [out_dim] calculated properly for model dims?

1.6 
consider try another optimaizer then Adam 
mlp_wrapper\main : optimizer = torch.optim.Adam(model.parameters(), lr=leaning_rate, weight_decay=reg, amsgrad=False) 
